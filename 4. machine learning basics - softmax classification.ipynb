{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  [1.0918864]\n",
      "loss:  [1.0076835]\n",
      "loss:  [0.96180207]\n",
      "loss:  [0.92809641]\n",
      "loss:  [0.8594355]\n",
      "loss:  [0.82788664]\n",
      "loss:  [0.81425363]\n",
      "loss:  [0.78638428]\n",
      "loss:  [0.73824936]\n",
      "loss:  [0.75816405]\n",
      "loss:  [0.70593011]\n",
      "loss:  [0.65243173]\n",
      "loss:  [0.68002439]\n",
      "loss:  [0.65541506]\n",
      "loss:  [0.63412756]\n",
      "loss:  [0.62858719]\n",
      "loss:  [0.61165625]\n",
      "loss:  [0.58983636]\n",
      "loss:  [0.6388666]\n",
      "loss:  [0.56678194]\n",
      "loss:  [0.54201567]\n",
      "loss:  [0.56649756]\n",
      "loss:  [0.60231709]\n",
      "loss:  [0.5364992]\n",
      "loss:  [0.59488243]\n",
      "loss:  [0.54561806]\n",
      "loss:  [0.50032568]\n",
      "loss:  [0.53931218]\n",
      "loss:  [0.55437028]\n",
      "loss:  [0.5170095]\n",
      "loss:  [0.53963232]\n",
      "loss:  [0.51689804]\n",
      "loss:  [0.50891566]\n",
      "loss:  [0.48621696]\n",
      "loss:  [0.49127173]\n",
      "loss:  [0.47326669]\n",
      "loss:  [0.52866369]\n",
      "loss:  [0.47955713]\n",
      "loss:  [0.49915311]\n",
      "loss:  [0.5243125]\n",
      "loss:  [0.48467645]\n",
      "loss:  [0.44337234]\n",
      "loss:  [0.44426566]\n",
      "loss:  [0.48620296]\n",
      "loss:  [0.45069274]\n",
      "loss:  [0.50417536]\n",
      "loss:  [0.4898037]\n",
      "loss:  [0.43416661]\n",
      "loss:  [0.49357334]\n",
      "loss:  [0.45336398]\n",
      "loss:  [0.43286693]\n",
      "loss:  [0.43442667]\n",
      "loss:  [0.43718526]\n",
      "loss:  [0.37249646]\n",
      "loss:  [0.43191069]\n",
      "loss:  [0.44228825]\n",
      "loss:  [0.37556678]\n",
      "loss:  [0.45620576]\n",
      "loss:  [0.45212314]\n",
      "loss:  [0.37420493]\n",
      "loss:  [0.42662707]\n",
      "loss:  [0.44410256]\n",
      "loss:  [0.41442043]\n",
      "loss:  [0.4319056]\n",
      "loss:  [0.38302517]\n",
      "loss:  [0.34785661]\n",
      "loss:  [0.43888408]\n",
      "loss:  [0.40003562]\n",
      "loss:  [0.37252083]\n",
      "loss:  [0.41303864]\n",
      "loss:  [0.43927726]\n",
      "loss:  [0.37483346]\n",
      "loss:  [0.41683403]\n",
      "loss:  [0.42980376]\n",
      "loss:  [0.36302418]\n",
      "loss:  [0.42494515]\n",
      "loss:  [0.42374325]\n",
      "loss:  [0.35865659]\n",
      "loss:  [0.39432132]\n",
      "loss:  [0.39671868]\n",
      "loss:  [0.39798951]\n",
      "loss:  [0.40286645]\n",
      "loss:  [0.38682923]\n",
      "loss:  [0.37930533]\n",
      "loss:  [0.34163308]\n",
      "loss:  [0.39972672]\n",
      "loss:  [0.36294368]\n",
      "loss:  [0.38053757]\n",
      "loss:  [0.37064379]\n",
      "loss:  [0.33451793]\n",
      "loss:  [0.39175037]\n",
      "loss:  [0.36787266]\n",
      "loss:  [0.34462151]\n",
      "loss:  [0.36046439]\n",
      "loss:  [0.37391865]\n",
      "loss:  [0.34497046]\n",
      "loss:  [0.37634131]\n",
      "loss:  [0.36604661]\n",
      "loss:  [0.32336876]\n",
      "loss:  [0.37272653]\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros([4, 3]), name='weights')\n",
    "b = tf.Variable(tf.zeros([3]), name='bias')\n",
    "\n",
    "\n",
    "# former inference is now used for combining inputs\n",
    "def combine_inputs(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "\n",
    "# define the training loop operations\n",
    "def inference(X):\n",
    "    # computer inference model over data X and return the result\n",
    "    return tf.nn.softmax(combine_inputs(X))\n",
    "    \n",
    "\n",
    "def loss(X, Y):\n",
    "    # computer loss over training data X and expected outputs Y\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(combine_inputs(X), Y))\n",
    "\n",
    "\n",
    "def read_csv(batch_size, filename, record_defaults):\n",
    "    # https://www.tensorflow.org/how_tos/reading_data/\n",
    "    filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), filename)])\n",
    "    \n",
    "    reader = tf.TextLineReader(skip_header_lines=0)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    # decode_csv will convert a Tensor from type string (the text line) in\n",
    "    # a tuple of tensor columns with the specified defaults, which also,\n",
    "    # sets the data type for each column\n",
    "    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\n",
    "    return tf.train.shuffle_batch(decoded, batch_size=batch_size,\n",
    "                                  capacity=batch_size * 50, min_after_dequeue=batch_size)\n",
    "\n",
    "\n",
    "def inputs():\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "    sepal_length, sepal_width, petal_length, petal_width, label = \\\n",
    "        read_csv(100, \"iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "\n",
    "    # convert class names to a 0 based class index\n",
    "    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.pack([\n",
    "                        tf.equal(label, [\"Iris-setosa\"]),\n",
    "                        tf.equal(label, [\"Iris-versicolor\"]),\n",
    "                        tf.equal(label, [\"Iris-virginica\"])\n",
    "                    ])), 0))\n",
    "\n",
    "    # pack all the features that we care about in a single matrix;\n",
    "    # we then transpose to have a matrix with one example per row and one feature per column.\n",
    "    features = tf.transpose(tf.pack([sepal_length, sepal_width, petal_length, petal_width])) \n",
    "\n",
    "    return features, label_number\n",
    "\n",
    "\n",
    "def train(total_loss):\n",
    "    # train / adjust model parameters according to computed total loss\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    # evaluate the resulting trained model\n",
    "    predicted = tf.cast(tf.argmax(inference(X), 1), tf.int32)\n",
    "    print sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32)))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# launch the graph in a session, setup boilerplate\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    X, Y = inputs()\n",
    "    \n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    initial_step = 0 \n",
    "    \n",
    "    # verify if we don't have a checkpoint saved already\n",
    "    #ckpt = tf.train.get_checkpoint_state(os.path.dirname(\".\"))\n",
    "    #if ckpt and ckpt.model_checkpoint_path:\n",
    "    #    #Restores from checkpoint\n",
    "    #    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #    initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "        \n",
    "    training_steps = 1000\n",
    "    for step in range(initial_step, training_steps):\n",
    "        sess.run([train_op])\n",
    "        \n",
    "        #if step % 1000 == 0:\n",
    "        #    saver.save(sess, 'my-model', global_step=step)\n",
    "            \n",
    "        # for debugging and learning purposes, see how the loss gets decremented through training steps.\n",
    "        if step % 10 == 0:\n",
    "            print \"loss: \", sess.run([total_loss])\n",
    "                \n",
    "    evaluate(sess, X, Y)\n",
    "    \n",
    "    #saver.save(sess, 'my-model', global_step=training_steps)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
