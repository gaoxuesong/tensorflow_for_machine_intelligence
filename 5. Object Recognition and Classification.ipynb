{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* example of tf.nn.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_batch = tf.constant([\n",
    "        [\n",
    "            # first input\n",
    "            [[0.0], [1.0]],\n",
    "            [[2.0], [3.0]]\n",
    "        ],\n",
    "        [\n",
    "            # second input\n",
    "            [[2.0], [4.0]],\n",
    "            [[6.0], [8.0]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "kernel = tf.constant([\n",
    "        [\n",
    "            [[1.0, 2.0]]\n",
    "        ]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0.,   0.],\n",
       "         [  1.,   2.]],\n",
       "\n",
       "        [[  2.,   4.],\n",
       "         [  3.,   6.]]],\n",
       "\n",
       "\n",
       "       [[[  2.,   4.],\n",
       "         [  4.,   8.]],\n",
       "\n",
       "        [[  6.,  12.],\n",
       "         [  8.,  16.]]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.], dtype=float32), array([ 3.,  6.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input pixel과 tf.nn.conv2d에서 filter를 거친 pixel은 같은 index를 갖는다.\n",
    "lower_right_image_pixel = sess.run(input_batch)[0][1][1]\n",
    "lower_right_kernel_pixel = sess.run(conv2d)[0][1][1]\n",
    "\n",
    "lower_right_image_pixel, lower_right_kernel_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.20000005],\n",
       "         [ 8.19999981]],\n",
       "\n",
       "        [[ 2.79999995],\n",
       "         [ 8.80000019]]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6x6x1 이미지에 3x3x1 커널 적용 예제\n",
    "input_batch = tf.constant([\n",
    "        [  # First Input (6x6x1)\n",
    "            [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],\n",
    "            [[0.1], [1.1], [2.1], [3.1], [4.1], [5.1]],\n",
    "            [[0.2], [1.2], [2.2], [3.2], [4.2], [5.2]],\n",
    "            [[0.3], [1.3], [2.3], [3.3], [4.3], [5.3]],\n",
    "            [[0.4], [1.4], [2.4], [3.4], [4.4], [5.4]],\n",
    "            [[0.5], [1.5], [2.5], [3.5], [4.5], [5.5]],\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "kernel = tf.constant([  # Kernel (3x3x1)\n",
    "        [[[0.0]], [[0.5]], [[0.0]]],\n",
    "        [[[0.0]], [[1.0]], [[0.0]]],\n",
    "        [[[0.0]], [[0.5]], [[0.0]]]\n",
    "    ])\n",
    "\n",
    "# NOTE: the change in the size of the strides parameter.\n",
    "conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 3, 3, 1], padding='SAME')\n",
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2, -1,  0,  1,  2], dtype=int32), array([0, 0, 0, 1, 2], dtype=int32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu example\n",
    "features = tf.range(-2, 3)\n",
    "sess.run([features, tf.nn.relu(features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.,  0.,  1.,  2.], dtype=float32),\n",
       " array([ 0.26894143,  0.5       ,  0.7310586 ,  0.88079703], dtype=float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid\n",
    "features = tf.to_float(tf.range(-1, 3))\n",
    "sess.run([features, tf.sigmoid(features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.,  0.,  1.,  2.], dtype=float32),\n",
       " array([-0.76159418,  0.        ,  0.76159418,  0.96402758], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tanh\n",
    "features = tf.to_float(tf.range(-1, 3))\n",
    "sess.run([features, tf.tanh(features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.1,  0. ,  0.1,  0.2], dtype=float32),\n",
       " array([-0.2       ,  0.        ,  0.        ,  0.40000001], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout\n",
    "features = tf.constant([-0.1, 0.0, 0.1, 0.2])\n",
    "sess.run([features, tf.nn.dropout(features, keep_prob=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.5]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_pool\n",
    "batch_size = 1\n",
    "input_height = 3\n",
    "input_width = 3\n",
    "input_channels = 1\n",
    "\n",
    "layer_input = tf.constant([\n",
    "        [\n",
    "            [[1.0], [0.2], [1.5]],\n",
    "            [[0.1], [1.2], [1.4]],\n",
    "            [[1.1], [0.4], [0.4]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "kernel = [batch_size, input_height, input_width, input_channels]\n",
    "max_pool = tf.nn.max_pool(layer_input, kernel, [1, 1, 1, 1], \"VALID\")\n",
    "sess.run(max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.5]]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_pool\n",
    "batch_size = 1\n",
    "input_height = 3\n",
    "input_width = 3\n",
    "input_channels = 1\n",
    "\n",
    "layer_input = tf.constant([\n",
    "        [\n",
    "            [[1.0], [1.0], [1.0]],\n",
    "            [[1.0], [0.5], [0.0]],\n",
    "            [[0.0], [0.0], [0.0]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "kernel = [batch_size, input_height, input_width, input_channels]\n",
    "avg_pool = tf.nn.avg_pool(layer_input, kernel, [1, 1, 1, 1], \"VALID\")\n",
    "sess.run(avg_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 1.]],\n",
       " \n",
       "         [[ 2.]],\n",
       " \n",
       "         [[ 3.]]]], dtype=float32), array([[[[ 0.70710677]],\n",
       " \n",
       "         [[ 0.89442718]],\n",
       " \n",
       "         [[ 0.94868326]]]], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.nn.local_response_normalization\n",
    "# [batch, image_height, image_width, image_channel]\n",
    "layer_input = tf.constant([\n",
    "        [[[1.0]], [[2.]], [[3.]]]\n",
    "    ])\n",
    "\n",
    "lrn = tf.nn.local_response_normalization(layer_input)\n",
    "sess.run([layer_input, lrn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* high level layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "         [ 319.33734131,    0.        ,    5.65779114,  104.56098938],\n",
       "         [ 151.72480774,   16.78249741,    0.        ,  158.22753906]],\n",
       "\n",
       "        [[ 131.75291443,    0.        ,  135.71949768,    0.        ],\n",
       "         [  68.17230988,    0.        ,    0.        ,    9.21641922],\n",
       "         [ 131.75291443,    0.        ,  135.71949768,    0.        ]],\n",
       "\n",
       "        [[ 151.72480774,   16.78249741,    0.        ,  158.22753906],\n",
       "         [ 319.33734131,    0.        ,    5.65779114,  104.56098938],\n",
       "         [   0.        ,    0.        ,    0.        ,    0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input = tf.constant([\n",
    "        [\n",
    "            [[0., 0., 0.], [255., 255., 255.], [254., 0., 0.]],\n",
    "            [[0., 191., 0.], [3., 108., 233.], [0., 191., 0.]],\n",
    "            [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "# tf.nn.conv2d의 higher version\n",
    "conv2d = tf.contrib.layers.convolution2d(\n",
    "            image_input,\n",
    "            num_outputs=4,\n",
    "            kernel_size=(1,1), # filter height, width\n",
    "            activation_fn=tf.nn.relu,\n",
    "            stride=(1,1), # skips the stride values for image_batch and input_channels\n",
    "            trainable=True)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.06500373],\n",
       "        [ 0.        ,  0.18417723]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.contrib.layers.fully_connected\n",
    "features = tf.constant([\n",
    "        [[1.2], [3.4]]\n",
    "    ])\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(features, num_outputs=2)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* image loading 후 TFRecord로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_filename = \"/Users/jackleg/study/tensorflow_for_mi/test-input-image-large.jpg\"\n",
    "filename_queue = tf.train.string_input_producer([image_filename])\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "image = tf.image.decode_jpeg(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 자꾸 hang된다. 왜 그러지?\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # 이미지를 TFRecord 포맷으로 저장하기\n",
    "    image_label = b'\\x01'\n",
    "    \n",
    "    image_loaded = sess.run(image)\n",
    "    image_bytes = image_loaded.tobytes()\n",
    "    image_height, image_width, image_channels = image_loaded.shape\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(\"./output/training-image.tfrecords\")\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),\n",
    "                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "            }))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]\n",
      "\n",
      " [[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]\n",
      "\n",
      " [[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]\n",
      "\n",
      " ..., \n",
      " [[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]\n",
      "\n",
      " [[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]\n",
      "\n",
      " [[ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  ..., \n",
      "  [ True  True  True]\n",
      "  [ True  True  True]\n",
      "  [ True  True  True]]]\n",
      "\u0001\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"./output/training-image.tfrecords\"])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # 저장한 TFRecord 읽기\n",
    "    #tf_record_filename_queue = tf.train.string_input_producer(\n",
    "    #                                tf.train.match_filenames_once(\"./output/training-image.tfrecords\"))\n",
    "    \n",
    "    tf_record_reader = tf.TFRecordReader()\n",
    "    #_, tf_record_serialized = tf_record_reader.read(tf_record_filename_queue)\n",
    "    _, tf_record_serialized = tf_record_reader.read(filename_queue)\n",
    "    \n",
    "    tf_record_features = tf.parse_single_example(\n",
    "                            tf_record_serialized,\n",
    "                            features={\n",
    "                                'label': tf.FixedLenFeature([], tf.string),\n",
    "                                'image': tf.FixedLenFeature([], tf.string)\n",
    "                            })\n",
    "    \n",
    "    tf_record_image = tf.decode_raw(tf_record_features['image'], tf.uint8)\n",
    "    tf_record_image = tf.reshape(tf_record_image,\n",
    "                                 [image_height, image_width, image_channels])\n",
    "    \n",
    "    tf_record_label = tf.cast(tf_record_features['label'], tf.string)\n",
    "    \n",
    "    # 원래 이미지와 TFRecord에서 읽은 이미지가 같은지 여부 확인\n",
    "    print sess.run(tf.equal(image, tf_record_image))\n",
    "    print sess.run(tf_record_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  4 109 234]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    print sess.run(tf.image.central_crop(image, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0]]\n",
      "\n",
      " [[0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    real_image = sess.run(image)\n",
    "\n",
    "    bounding_crop = tf.image.crop_to_bounding_box(\n",
    "                        real_image, offset_height=0, offset_width=0, target_height=2, target_width=1)\n",
    "    \n",
    "    print sess.run(bounding_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [254   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [254   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [254   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " ..., \n",
      " [[254   0   0]\n",
      "  [254   0   0]\n",
      "  [254   0   0]\n",
      "  ..., \n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    real_image = sess.run(image)\n",
    "    \n",
    "    pad = tf.image.pad_to_bounding_box(\n",
    "                    real_image, offset_height=0, offset_width=0, target_height=101, target_width=101)\n",
    "    \n",
    "    print sess.run(pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.image.resize_image_with_crop_or_pad()를 사용하면 이미지를 crop하고 난 후에 padding으로 resizing할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* flipping\n",
    " * 이미지 중 일부를 좌우로 flipping한 후 다시 위아래로 flipping.\n",
    " * tf.image.random_flip_left_right, tf.image.random_flip_up_down을 사용하면 랜덤하게 이미지를 flip할 수 있음.\n",
    " * 좀 더 다양한 training 데이터를 만들기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8), array([[[0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    top_left_pixels = tf.slice(image, [0, 0, 0], [2, 2, 3])\n",
    "    \n",
    "    flip_horizon = tf.image.flip_left_right(top_left_pixels)\n",
    "    flip_vertical = tf.image.flip_up_down(flip_horizon)\n",
    "    \n",
    "    print sess.run([top_left_pixels, flip_vertical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* control saturation, hue, contrast, brightness\n",
    " * tf.image.adjust_contrast, tf.image.adjust_hue, tf.image.adjust_saturation 등의 함수를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 254.19999695    2.20000005   15.19999981]\n"
     ]
    }
   ],
   "source": [
    "# 이미지를 0.2만큼 밝게 만든다.\n",
    "with tf.Session() as sess:\n",
    "    example_red_pixel = tf.constant([254., 2., 15.])\n",
    "    adjust_brightness = tf.image.adjust_brightness(example_red_pixel, 0.2)\n",
    "\n",
    "    print sess.run(adjust_brightness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지의 color scheme을 gray scale로 전환하기.\n",
    " * tf.image.rgb_to_grayscale()을 사용\n",
    " \n",
    "* rgb를 hsv로 변경하기.\n",
    " * tf.image.rgb_to_hsv()를 사용.\n",
    " * hsv는 실수값을 사용하기 때문에 rgb의 경우 tf.image.convert_image_dtype을 이용해 값을 tf.float32로 변경해서 사용.\n",
    " \n",
    "* 위 함수들과 반대로 tf.image.xxx_to_rgb 함수도 존재함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download images from http://vision.stanford.edu/aditya86/ImageNetDogs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. convert images to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "\n",
    "# sampleing용으로 일부 데이터만 사용.\n",
    "image_filenames = glob.glob(\"./images/n02085*/*.jpg\")\n",
    "\n",
    "training_dataset = defaultdict(list)\n",
    "testing_dataset = defaultdict(list)\n",
    "\n",
    "# Split up the filename into its breed and corresponding filename. The breed is found by taking the directory name.\n",
    "image_filename_with_breed = map(lambda filename: (filename.split(\"/\")[2], filename), image_filenames)\n",
    "\n",
    "# group each image by the breed which is the 0th element in the tuple returned above.\n",
    "for dog_breed, breed_images in groupby(image_filename_with_breed, lambda x: x[0]):\n",
    "    # Enumerate each breed's image and send ~20% of the images to 4 testing set.\n",
    "    for i, breed_image in enumerate(breed_images):\n",
    "        if i % 5 == 0:\n",
    "            testing_dataset[dog_breed].append(breed_image[1])\n",
    "        else:\n",
    "            training_dataset[dog_breed].append(breed_image[1])\n",
    "            \n",
    "    # check that each breed includes at least 18% of the images for testing\n",
    "    breed_training_count = len(training_dataset[dog_breed])\n",
    "    breed_testing_count = len(testing_dataset[dog_breed])\n",
    "    \n",
    "    assert round(breed_testing_count*1.0 / (breed_training_count + breed_testing_count), 2) > 0.18, \"Not enough testing images.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write testing files.\n",
      "processing n02085620-Chihuahua...\n",
      "processing n02085936-Maltese_dog...\n",
      "processing n02085782-Japanese_spaniel...\n",
      "write training files.\n",
      "processing n02085620-Chihuahua...\n",
      "processing n02085936-Maltese_dog...\n",
      "processing n02085782-Japanese_spaniel...\n"
     ]
    }
   ],
   "source": [
    "def write_records_file(dataset, record_location):\n",
    "    \"\"\"\n",
    "    Fill a TFRecords file with the images found in `dataset` and include their category.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: dict(list)\n",
    "        Dictionary with each key being a label for the list of image filenames of its value.\n",
    "    record_location: str\n",
    "        Location to store the TFRecord output.\n",
    "    \"\"\"\n",
    "    \n",
    "    writer = None\n",
    "    \n",
    "    # Enumerating the dataset because the current index is used to breakup the files if they get over 100\n",
    "    # images to avoid a slowdown in writing.\n",
    "    current_index = 0\n",
    "    for breed, images_filenames in dataset.items():\n",
    "        print \"processing %s...\" % breed\n",
    "        \n",
    "        for image_filename in images_filenames:\n",
    "            if current_index % 100 == 0:\n",
    "                if writer:\n",
    "                    writer.close()\n",
    "                    \n",
    "                record_filename = \"{record_location}-{current_index}.tfrecords\".format(\n",
    "                                        record_location=record_location,\n",
    "                                        current_index=current_index)\n",
    "                \n",
    "                writer = tf.python_io.TFRecordWriter(record_filename)\n",
    "            current_index += 1\n",
    "            \n",
    "            image_file = tf.read_file(image_filename)\n",
    "            \n",
    "            # In ImageNet dogs, there are a few images which TensorFlow doesn't recognize as JPEGs. This\n",
    "            # try/catch will ignore those images.\n",
    "            try:\n",
    "                image = tf.image.decode_jpeg(image_file)\n",
    "            except:\n",
    "                print(image_filename)\n",
    "                continue\n",
    "            \n",
    "            # Converting to grayscale saves processing and memory but isn't required.\n",
    "            grayscale_image = tf.image.rgb_to_grayscale(image)\n",
    "            resized_image = tf.image.resize_images(grayscale_image, [250, 151])\n",
    "            \n",
    "            # tf.cast is used here because the resized images are floats but haven't been converted into\n",
    "            # image floats where an RGB value is between [0, 1).\n",
    "            image_bytes = sess.run(tf.cast(resized_image, tf.uint8)).tobytes()\n",
    "            \n",
    "            # instead of using the label as a string, it'd be more efficient to trun it into either an\n",
    "            # integer index or a one-hot encoded rank one tensor.\n",
    "            # https://en.wikipedia.org/wiki/One-hot\n",
    "            image_label = breed.encode(\"utf-8\")\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),\n",
    "                        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "                    }))\n",
    "            \n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "print \"write testing files.\"\n",
    "write_records_file(testing_dataset, \"./output/testing-images/testing-image\")\n",
    "\n",
    "print \"write training files.\"\n",
    "write_records_file(training_dataset, \"./output/training-images/training-image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. load images\n",
    " * TFRecord 포맷으로 저장한 이미지 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "                    tf.train.match_filenames_once(\"./output/training-images/*.tfrecords\"))\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "                serialized,\n",
    "                features={\n",
    "                    'label': tf.FixedLenFeature([], tf.string),\n",
    "                    'image': tf.FixedLenFeature([], tf.string),\n",
    "           })\n",
    "\n",
    "record_image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "# Changing the image into this shape helps train and visualize the output by converting it to\n",
    "# be organized like an image.\n",
    "image = tf.reshape(record_image, [250, 151, 1])\n",
    "\n",
    "label = tf.cast(features['label'], tf.string)\n",
    "\n",
    "min_after_dequeue = 10\n",
    "batch_size = 3\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "image_batch, label_batch = tf.train.shuffle_batch(\n",
    "                                [image, label],\n",
    "                                batch_size=batch_size,\n",
    "                                capacity=capacity,\n",
    "                                min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model\n",
    " * mnist convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(3), Dimension(125), Dimension(76), Dimension(32)]),\n",
       " TensorShape([Dimension(3), Dimension(63), Dimension(38), Dimension(32)]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the images to a float of [0, 1) to match the expected input to convolution2d\n",
    "float_image_batch = tf.image.convert_image_dtype(image_batch, tf.float32)\n",
    "\n",
    "conv2d_layer_one = tf.contrib.layers.convolution2d(\n",
    "                        float_image_batch,\n",
    "                        num_outputs=32, # The number of filters to generate\n",
    "                        kernel_size=(5, 5), # It's only the filter height and width.\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.random_normal_initializer(),\n",
    "                        stride=(2, 2),\n",
    "                        trainable=True)\n",
    "pool_layer_one = tf.nn.max_pool(conv2d_layer_one, ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "\n",
    "# Note, The first and last dimension of the convolution output hasn't changed but the middle two dimensions have.\n",
    "conv2d_layer_one.get_shape(), pool_layer_one.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(3), Dimension(63), Dimension(38), Dimension(64)]),\n",
       " TensorShape([Dimension(3), Dimension(32), Dimension(19), Dimension(64)]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_layer_two = tf.contrib.layers.convolution2d(\n",
    "                        pool_layer_one,\n",
    "                        num_outputs=64,\n",
    "                        kernel_size=(5, 5),\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.random_normal_initializer(),\n",
    "                        stride=(1, 1),\n",
    "                        trainable=True)\n",
    "\n",
    "pool_layer_two = tf.nn.max_pool(conv2d_layer_two,\n",
    "                                ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "\n",
    "conv2d_layer_two.get_shape(), pool_layer_two.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(38912)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fully connected layer를 만들기 위해 마지막 layer를 변형시킴.\n",
    "# first dimension: seperate each image\n",
    "# second dimension: rank one tensor of each input tensor.\n",
    "flattened_layer_two = tf.reshape(\n",
    "                            pool_layer_two,\n",
    "                            [\n",
    "                                batch_size, # Each image in the image_batch\n",
    "                                -1, # Every other dimension of the input\n",
    "                            ])\n",
    "\n",
    "flattened_layer_two.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_three = tf.contrib.layers.fully_connected(\n",
    "                        flattened_layer_two,\n",
    "                        512,\n",
    "                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                        activation_fn=tf.nn.relu)\n",
    "\n",
    "# dropout some of the neurons, reducing their importance in the model\n",
    "hidden_layer_three = tf.nn.dropout(hidden_layer_three, 0.1)\n",
    "\n",
    "# the output of this are all the connections between the previous layers and the 120 different dog breeds\n",
    "# available to train on.\n",
    "# (여기에서 나는 3개만 사용함.)\n",
    "final_fully_connected = tf.contrib.layers.fully_connected(\n",
    "                            hidden_layer_three,\n",
    "                            120, # number of dog breeds in the ImageNet Dogs dataset\n",
    "                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# breed 이름을 정수에 매핑시키기 위해 tf.map_fn을 사용.\n",
    "import glob\n",
    "\n",
    "# Find every directory name in the imagenet-dogs directory\n",
    "labels = list(map(lambda c: c.split(\"/\")[-1], glob.glob(\"./images/*\")))\n",
    "\n",
    "# Match every label from label_batch and return the index where they exist in the list of classes.\n",
    "train_labels = tf.map_fn(lambda l: tf.where(tf.equal(labels, l))[0,0:1][0], label_batch, dtype=tf.int64)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
